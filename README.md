# Feature tracking algorithm based on event-camera using a multithreading approach

This repository implements an extension of the algorithm described in the 2019 IJCV paper [**EKLT: Asynchronous, Photometric Feature Tracking using Events and Frames**](http://rpg.ifi.uzh.ch/docs/IJCV19_Gehrig.pdf) by [Daniel Gehrig](https://danielgehrig18.github.io/), [Henri Rebecq](http://henri.rebecq.fr), [Guillermo Gallego](http://www.guillermogallego.es), and [Davide Scaramuzza](http://rpg.ifi.uzh.ch/people_scaramuzza.html) using a multithreading approach.


## Overview

This algorithm is based on EKLT, that works by leveraging the complementarity between events and frames for feature tracking. Using the events, it manages to track in the blind-time between two frames. First features are extracted on the frames and then tracked using only the events. The tracker then produces asynchronous feature tracks with high temporal resolution. More 
details can be found in the [paper](http://rpg.ifi.uzh.ch/docs/IJCV19_Gehrig.pdf) and in the [repository](https://github.com/uzh-rpg/rpg_eklt). Our job was to make the algorithm more efficient, making it multithreaded, and making it more suitable for a possible real application of visual odometry.
For more details look at the [report](report.pdf) of our work. It is written in Italian for didactic reasons.

## Installation and running

For the installation refer to what is described in the eklt [repository](https://github.com/uzh-rpg/rpg_eklt). However, there are some differences. Inside folder `config`, there is a file named `eklt.conf`, which now contains a parameter that allows you to specify the number of threads, and two parameters that enable you to handle the memory used by the events vector. In addition, in order to build the project now you have to use the command `catkin build eklt_multithreading` instead of `catkin build eklt`. And finally, to launch it you should run `roslaunch eklt eklt_multithreading.launch` instead of `roslaunch eklt eklt.launch`


## Evaluation

The tests were performed with different thread numbers, and all of them were performed using Ubuntu 16.04 and ROS Kinetic. For the sake of simplicity, the algorithm was tested only on the first 10 seconds of the rosbag data.
The tracks file generated by the execution of the algorithm contains all the information needed to reconstruct the tracks of the features. The feature tracks are stored using the following format:

|feature id| timestamp          | x     | y     |
|:--------:|:------------------:|:-----:|:-----:|
|0         |1468940293.922985274|187.032|132.671|
|2         |1468940293.958816290|204.603|105.36 |
|1         |1468940293.957388878|222.378|104.176|
|0         |1468940293.964686394|223.596|19.1384|
|1         |1468940293.960546732|182.122|52.1019|
|2         |1468940293.960608244|172.227|47.1469|

The performances observed using different thread numbers are summarized in the following table. 
![Performance](/images/tabella_ris.png)

The following graph shows the trend of the seconds in relation to the number of threads.
![Seconds in relation to the number of threads](/images/grafico_ris.png)



## Additional Resources on Event Cameras

* [Event-based Vision Survey](http://rpg.ifi.uzh.ch/docs/EventVisionSurvey.pdf)
* [List of Event-based Vision Resources](https://github.com/uzh-rpg/event-based_vision_resources)
* [Event Camera Dataset](http://rpg.ifi.uzh.ch/davis_data.html)
* [Event Camera Simulator](http://rpg.ifi.uzh.ch/esim)
* [RPG research page on Event Cameras](http://rpg.ifi.uzh.ch/research_dvs.html)
* [EKLT Conference paper, ECCV'18](http://rpg.ifi.uzh.ch/docs/ECCV18_Gehrig.pdf)
